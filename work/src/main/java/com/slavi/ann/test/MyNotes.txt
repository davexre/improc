Octave formulas for LSA debug
=============================
N=A'*A;
max(max(abs(N)))
max(max(abs(A)))


Interesting to read
===================
CNN:
	https://grzegorzgwardys.wordpress.com/2016/04/22/8/
	http://www.jefkine.com/general/2016/09/05/backpropagation-in-convolutional-neural-networks/
	
https://en.wikipedia.org/wiki/Basis_function

Functions & derivatives
=======================

*** Sigmoid
s(x) = 1 / (1+e^(-x))
ds/dx = s(1-s)

*** Applying layer weights
o - output
x - input
w[x,o] - weight between input x and output o

o(x) = sum( x*w[x,o] )
do/dx = w[x,o]
do/dw = x

*** Training error
e - error
v - last layer output
t - target value

e(v) = sum( (t-v)^2 ) / 2
de/dv = v-t

*** Layer & sigmoid
x - input
w[x,o] - weight between input x and output o
o - layer weight sum of inputs
v - layer output (that can be used as input for the next layer or if last layer compared to a target)

o(x) = sum( x*w[x,o] )
v(x) = s(o(x)) = 1 / (1+e^(-o(x)))
dv/dx = (dv/do)*(do/dx) = (ds/do)*(do/dx) = s * (1-s) * w[x,o]
dv/dw = (dv/do)*(do/dw) = (ds/do)*(do/dw) = s * (1-s) * x

*** Convolution layer
v - layer after activation function is applied
o - output
x - input
k[a,b] - kernel weights. The kernel has size a by b

v(x) = s(o(x))
o(x) = conv2d( x, k )
o[i,j](x) = sum_a(sum_b( k[a,b] * x[i+a,k+b] ))
do/dx = ?
do/dw = ?

*** Trainer and train thresholds
Oi - Output neuron i, 0 <= Oi <= 1
Oi(k) - Output neuron i for pattern k
Ti - Target for output neuron i
Ti(k) - Target for output neuron i for train pair k
K - Number of train pairs - max(k)

Ei = abs(Ti - Oi)
MaxErr = max(Ei(k)) # for all i,k
AvgErr = sum(Ei(k)) / K
LearnProgress = AvgErr[previous epoch] - AvgErr[current epoch]
MaxPatternError(k) = max(Ei(k))
PatternsLearned = sum( MaxPatternError(k) < 0.15 ? 1 : 0 ) / K

Stop training when
MaxError < 0.2
LearnProgress < 0.0002
PatternsLearned > 0.80 and MaxError < 0.3
